{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b52fa3e-e433-4df6-b45e-984a9b555fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2762e6-fa27-4023-a73c-c3139d48346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14444eca-8b30-41da-a031-6026ff587691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baaeb437-9735-40ac-bd82-cd83b36040e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb0f613-29b5-4ee4-8cee-b5767f75c8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6417b1c-eb5f-427d-9a6d-3313aaf3901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will keep a counter b for each bigram\n",
    "b = {}\n",
    "\n",
    "for w in words:\n",
    "    # we will use <S> = start and <E> = end for each word\n",
    "    chs = ['<S>'] + list(w) + ['E']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # construct bigram of the two adjacent chars\n",
    "        bigram = (ch1, ch2)\n",
    "        # increment counter\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d8bf1-73da-443c-bcab-c13625e971c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebb107-523f-40ea-8aab-8b50550c2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dict by value in decreasing order\n",
    "sorted(b.items(), key = lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a5292fa-1550-430c-bc1d-fb73d26965e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2d97d-c349-4f08-b246-6f3bb915896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### represent this bigram counter as a 2D array (tensor)\n",
    "### we will use '.' as the start/end char\n",
    "\n",
    "# we will use int32 instead of float32 since all our values are ints\n",
    "# size is 27 (26 chars + 1 start/end char)\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "# get all possible values for a character in the names list\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# create a LUT char -> int\n",
    "stoi = {c: i+1 for i, c in enumerate(chars)}\n",
    "# add the start/end char\n",
    "stoi['.'] = 0\n",
    "\n",
    "# reverse lookup table to get int -> char LUT\n",
    "# offset all the i's by +1 since we want '.' to be at index 0\n",
    "itos = {c: i for i, c in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7be2a4-83bb-47d5-b0f0-25407f11a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### populate the 2D array counter\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # find indices with LUT\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4adcca-8775-4652-a4d6-17037b149716",
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize the bigram counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0465db1-4199-4585-b5da-f4f57b61e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5573f8f7-3c5f-4fbc-bc10-aca974f9e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can turn a distribution into a probability distribution by dividing by the sum\n",
    "p = N[0].float()\n",
    "p /= p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef8a76b9-a167-4fc7-97db-e723fa77d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use a generator to pick samples from a probability distribution\n",
    "# use the same seed for the generator produces deterministic results\n",
    "# g = torch.Generator().manual_seed(2147483647)\n",
    "# p = torch.rand(3, generator=g)\n",
    "# p /= p.sum()\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2aaade8-377d-417c-9a7d-c6f2895456c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an index\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420f354-af77-4c00-9003-afa6f21907c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "### GENERATE NAMES\n",
    "for i in range(10):\n",
    "    ### DEFINE SAMPLING LOOP\n",
    "    # start with ix = 0, since 0 -> '.' which is the start/end char\n",
    "    ix = 0\n",
    "    # output\n",
    "    out = []\n",
    "    while True:\n",
    "        # get the probability distrbution for the char after c = itos[ix]\n",
    "        p = N[ix].float()\n",
    "        # normalize the probability distribution\n",
    "        p = p / p.sum()\n",
    "        # sample for next char index\n",
    "        ix = torch.multinomial(P, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(out))\n",
    "\n",
    "# OPTIMIZATIONS\n",
    "#     - normalize each row of N before the loop to avoid performing the same division multiple times\n",
    "#     - use in-place division with the /= operator to avoid allocating memory for the additional p array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e47b24c4-31e6-4c7f-b063-c2074d0b1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMALIZE PROBABILITY MATRIX\n",
    "P = N.float()\n",
    "# we want to sum along the rows; the columns should be collapsed into a single column\n",
    "# row = 0, column = 1 => dim = 1\n",
    "# we also need keepdim=True, otherwise the dim=1 is flattened out, and we get a (27) tensor or a row vector\n",
    "P /= P.sum(1, keepdim=True)\n",
    "\n",
    "# more on broadcasting: https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "\n",
    "\n",
    "# we are trying to divide a (27, 27) tensor by a (27, 1) tensor\n",
    "\n",
    "# 27, 27\n",
    "# 27,  1\n",
    "\n",
    "# in this case, torch takes the tensor where the dim is 1,\n",
    "# and copies it horizontally 27 times \n",
    "\n",
    "# example:\n",
    "\n",
    "# a       a a a a  ...  a a a a\n",
    "# b       b b b b  ...  b b b b\n",
    "# .                 .\n",
    "# .   ->            .\n",
    "# .                 .\n",
    "# y       y y y y  ...  y y y y\n",
    "# z       z z z z  ...  z z z z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79598c4f-3c37-439b-8336-58ab50ab9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "### GENERATE NAMES\n",
    "for i in range(10):\n",
    "    ### DEFINE SAMPLING LOOP\n",
    "    # start with ix = 0, since 0 -> '.' which is the start/end char\n",
    "    ix = 0\n",
    "    # output\n",
    "    out = []\n",
    "    while True:\n",
    "        # get the normalized probability distrbution for the char after c = itos[ix]\n",
    "        p = P[ix]\n",
    "        # sample for next char index\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7f699ee0-fa7d-4e9a-982b-89f5129a2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_nll: 5.157317638397217\n"
     ]
    }
   ],
   "source": [
    "### EVALUATING THE LOSS\n",
    "\n",
    "# in maximum likelihood estimation, we typically use LIKELIHOOD\n",
    "# likelihood is product of the all the probabilities\n",
    "\n",
    "# since our probabilities are all less than 1, the product will be a very small number\n",
    "# instead, we will take the NEGATIVE LOG (NEGATIVE LOG LIKELIHOOD or NLL)\n",
    "# useful since log(a*b*c) = log(a) + log(b) + log(c); we can move between product and sum\n",
    "# we make it negative just to stay consistent with the semantics of a loss function, where low == good\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in [\"fdafsjq\"]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # find indices with LUT\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        # find probability of this bigram\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f'{ch1}{ch2}: {logprob:.4f}')\n",
    "        \n",
    "nll = -log_likelihood\n",
    "\n",
    "# the AVERAGE NLL is important, since each bigram will increase the loss\n",
    "avg_nll = nll / n\n",
    "\n",
    "print(f'avg_nll: {avg_nll}')\n",
    "\n",
    "# GOAL: maximize likelihood\n",
    "# -> maximize log likelihood (since log is increasing)\n",
    "# -> minimize negative log likelihood\n",
    "# -> minimize average log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "530e3afe-1f4a-4be8-8f00-f835413a6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PROBLEM!!!\n",
    "# if an input has a bigram in it that is not in the training set,\n",
    "# i.e. its count is 0,\n",
    "# we get log(0) = -inf which is very bad\n",
    "\n",
    "# to fix this, we can just add 1 to all the counts\n",
    "# this doesn't really change the probability distributions, but avoids the inf case\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
